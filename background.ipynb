{
 "metadata": {
  "name": "",
  "signature": "sha256:bda43f63b9457257b987c766c1d0febc6d17320d82fe41066a904dd2e84b63f2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Skalable K-means++"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scalable K-Means++"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "K-means clustering, for short, is to separate objects into k groups in which the members has the closest mean. K-means is one of the most popular clustering analysis, but one of the disadvantages is that it's easy to get stuck in local optimum. Therefore, the initial k points are critical for obtaining a good final result.\n",
      "\n",
      "K-means ++, an algorithm to choose the initial values of k-means clustering, has been proved that the initial values generated by this algorithm is close to global optimum. It is guaranteed to find a solution that is O(log k) competitive to the optimal k-means solution. However, it's not suitable for massive data, as we need to pass k times over the whole data set.\n",
      "\n",
      "Therefore, the author come up with a updated initialization algorithm, k-means||, to address this problem, which only needs logarithmic number of passes. This really interests me, because k-means clustering is really widely used in the data analysis. It's meaningful to figure out how to apply this to large-scale data sets which are increasingly prevalent. Besides, the algorithm is parallelizable, so that I can explore more and learn more about parallelization in python programming."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Algorithm (Pseudocode)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "k-means"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let X={$x_1,x_2,...x_n$} be the set of points in d-dimensional Euclidean space, and k is the number of clusters we will divide X into. This starts with randomly choosing k points from X as initial values of centers $c_1,...,c_k$. In each iteration, each point $x_i$ is assigned to the closest cluster by calculating $\\text{arg min}_j d(x_i,c_j)$. The calculate the new centroids of the observations in the new clusters as the k centers for the next iteration.\n",
      "$$centroid(X) = \\frac{1}{|X|}\\sum_{x \\in X} x$$\n",
      "The iteration is repeated until a stable set of centers is obtained."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "k-means++"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The main idea of k-means++ is to choose centers one by one in a controlled fashion, where the current set of chosen centers will influence the choice of next center. The algorithm is shown as below:\n",
      "1. C <- sample a point uniformly at random from X\n",
      "2. while |C|<k, do:\n",
      "3. $\\qquad$ sample x $\\in$ X with probability $\\frac{d^2(x,c)}{\\phi_x(c)}$, where\n",
      "$$\\phi_Y(C) = \\sum_{y\\in Y}d^2(y,C)$$\n",
      "4. $\\qquad$ C <- C $\\cup$ {x}\n",
      "5. end while"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "k-means ||"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. C <- smaple a point uniformly at random from X\n",
      "2. $\\psi$ <- $\\phi(C)$\n",
      "3. for O($\\log \\phi$) times do:\n",
      "4. $\\qquad$ C' <- sample each point x $\\in$ X independently with probability $p_x =\\frac{l \\cdot d^2(x,c)}{\\phi_x(c)}$ \n",
      "5. $\\qquad$ C <- C $\\cup$ C'\n",
      "5. end for\n",
      "7. For x $\\in$ C, set $w_x$ to be the number of points in X closer to x than any other point in C\n",
      "8. Recluster the weighted points in C into K clusters"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Advantages and Disadvantages"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}